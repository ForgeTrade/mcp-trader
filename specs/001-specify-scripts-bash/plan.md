# Implementation Plan: MCP Gateway System with Provider Orchestration

**Branch**: `001-specify-scripts-bash` | **Date**: 2025-10-18 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/001-specify-scripts-bash/spec.md`

**Note**: This plan was generated by the `/speckit.plan` command based on the feature specification and constitution requirements.

## Summary

Build a central MCP gateway (Python) that aggregates capabilities (tools, resources, prompts) from multiple microservice providers (Go/Rust) and exposes them through a unified MCP interface to AI clients. The gateway handles provider discovery, request routing, schema validation, authentication at the perimeter, and optional event streaming via message broker. Demo providers implement simple echo/sum tools to validate the architecture's language-agnostic contract.

**Primary Requirement**: Enable AI clients to invoke tools from heterogeneous microservices through a single MCP connection without managing multiple provider connections directly.

**Technical Approach**: Use MCP Python SDK for stdio transport, define gRPC provider contract for language-agnostic service interface, implement connection pooling and fail-fast timeout strategy for reliability, validate all payloads against JSON Schema, and optionally integrate NATS JetStream for event streaming.

## Technical Context

**Language/Version**:
- Gateway: Python 3.11+
- Provider 1: Go 1.21+
- Provider 2: Rust 1.75+

**Primary Dependencies**:
- Gateway: MCP Python SDK (`mcp` package), gRPC Python client, NATS Python client (optional), OpenTelemetry SDK
- Go Provider: gRPC Go, protobuf compiler
- Rust Provider: Tonic (gRPC), prost (protobuf), tokio runtime

**Storage**: N/A (stateless gateway, configuration from file/env)

**Testing**:
- Gateway: pytest, pytest-asyncio
- Go Provider: Go testing package, testify
- Rust Provider: cargo test, tokio-test
- Integration: MCP Inspector (visual testing tool)

**Target Platform**: Linux/macOS development environments, containerized deployment (Docker Compose for local, Kubernetes for production)

**Project Type**: Distributed system (multiple services) - gateway + providers architecture

**Performance Goals**:
- Tool invocation latency: <500ms p95 for simple operations
- Provider discovery: <3 seconds on gateway startup
- Event delivery: <2 seconds when streaming enabled
- Concurrent connections: 10-20 per provider via connection pooling

**Constraints**:
- Timeout: 2-3 seconds per tool invocation (fail-fast, no retries)
- Payload size: 10MB maximum for tool invocations
- Authentication: Perimeter-only (client-gateway), internal uses network isolation/mTLS
- Observability: Structured logging to stderr with correlation IDs

**Scale/Scope**:
- MVP: 1 gateway + 2 demo providers (Go, Rust)
- Expected load: Development/testing workload (low volume, <100 concurrent requests)
- Capability count: ~4 tools, 1 resource, 1 prompt per provider
- Event throughput: <10 events/second (optional feature)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Initial Check (Pre-Research)

| Principle | Status | Notes |
|-----------|--------|-------|
| **I. Simplicity and Readability** | ✅ PASS | Gateway uses simple proxy/routing logic; providers implement straightforward tool execution |
| **II. Library-First Development** | ✅ PASS | Using MCP Python SDK, gRPC libraries, NATS client - no custom protocol implementation |
| **III. Justified Abstractions** | ⚠️ REVIEW | Provider interface abstraction needed for multi-language support; gateway registry pattern for dynamic provider management - both justified by requirements |
| **IV. DRY Principle** | ✅ PASS | Shared protobuf definitions prevent duplication across providers; common validation logic centralized |
| **V. Service and Repository Patterns** | ✅ PASS | Gateway uses service pattern for provider management; no persistence layer needed (stateless) |
| **VI. 12-Factor Methodology** | ✅ PASS | Config from env/file, stateless processes, stdout logging, port binding for providers |
| **VII. Minimal OOP** | ✅ PASS | Python gateway can use procedural style with modules; Go/Rust naturally lean procedural/functional |

**Gate Decision**: ✅ **PROCEED TO PHASE 0**

**Justifications Required**:
- Provider interface abstraction: Needed to support multiple languages (Go, Rust, future) and enable hot-swapping providers
- Gateway registry pattern: Required for dynamic provider discovery and capability aggregation from configuration

### Post-Design Check (After Phase 1)

| Principle | Status | Notes |
|-----------|--------|-------|
| **I. Simplicity and Readability** | ✅ PASS | Data model uses simple entities with clear relationships; proto contract is straightforward; no deep nesting |
| **II. Library-First Development** | ✅ PASS | All technology choices leverage official SDKs (MCP, gRPC, Tonic); no custom protocol implementations |
| **III. Justified Abstractions** | ✅ PASS | Capability hierarchy (Tool/Resource/Prompt) justified by MCP spec requirements; Provider abstraction needed for multi-language support |
| **IV. DRY Principle** | ✅ PASS | Single shared protobuf contract prevents duplication; JSON schemas reused across providers |
| **V. Service and Repository Patterns** | ✅ PASS | Gateway uses service pattern for provider management; providers separate business logic from gRPC handlers |
| **VI. 12-Factor Methodology** | ✅ PASS | Stateless design confirmed; config from providers.yaml; structured logging to stderr; no persistence layer |
| **VII. Minimal OOP** | ✅ PASS | Data model defines entities as data structures, not classes; implementation can use procedural style |

**Gate Decision**: ✅ **APPROVED FOR IMPLEMENTATION**

**Design Validation**:
- All abstractions justified and documented in Complexity Tracking
- No new violations introduced during design phase
- Contracts follow industry standards (protobuf, JSON Schema 2020-12, CloudEvents 1.0)
- Architecture aligns with constitution principles

## Project Structure

### Documentation (this feature)

```
specs/001-specify-scripts-bash/
├── plan.md              # This file (/speckit.plan command output)
├── research.md          # Phase 0 output (/speckit.plan command)
├── data-model.md        # Phase 1 output (/speckit.plan command)
├── quickstart.md        # Phase 1 output (/speckit.plan command)
├── contracts/           # Phase 1 output (/speckit.plan command)
│   ├── provider.proto   # gRPC service definition
│   └── schemas/         # JSON Schema files for tools
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)

```
/mcp-gateway/                   # Python MCP server (central gateway)
  pyproject.toml                # uv-managed dependencies
  mcp_gateway/
    __init__.py
    main.py                     # stdio MCP server entry point
    providers_registry.py       # Provider discovery and management
    validation.py               # JSON Schema validation
    adapters/
      grpc_client.py            # gRPC client for provider communication
      nats_client.py            # (optional) NATS JetStream subscriber
  providers.yaml                # Static provider configuration
  tests/
    unit/                       # Unit tests for gateway logic
    integration/                # Integration tests with mock providers

/providers/
  /hello-go/                    # Go gRPC provider (demo)
    cmd/server/main.go          # gRPC server entry point
    internal/
      tools/
        echo.go                 # Echo tool implementation
        sum.go                  # Sum tool implementation
      resources/
        greeting.go             # Resource handler
      prompts/
        hello_plan.go           # Prompt template
    tests/
      integration_test.go       # Provider tests

  /hello-rs/                    # Rust gRPC provider (demo)
    src/
      main.rs                   # gRPC server entry point
      tools/
        echo.rs                 # Echo tool implementation
        sum.rs                  # Sum tool implementation
      resources/
        greeting.rs             # Resource handler
      prompts/
        hello_plan.rs           # Prompt template
    tests/
      integration.rs            # Provider tests

/pkg/
  /proto/                       # Shared protobuf definitions
    provider.proto              # Provider service contract
  /schemas/                     # Shared JSON Schema files
    echo.input.schema.json
    sum.input.schema.json

/infra/
  docker-compose.yml            # NATS + dev environment
  nats-setup/                   # JetStream stream/consumer config

/docs/
  provider-contract.md          # How to implement new providers
  mcp-surface.md                # Naming conventions for capabilities

Makefile                        # Build/run targets
```

**Structure Decision**:

This is a distributed system requiring multiple services (gateway + providers), so we use a **monorepo with multiple projects**:

1. **mcp-gateway/**: Python application using MCP SDK for stdio transport
2. **providers/hello-go/**: Go microservice implementing provider gRPC contract
3. **providers/hello-rs/**: Rust microservice implementing same contract (language-agnostic validation)
4. **pkg/**: Shared contract definitions (proto, schemas) to prevent duplication
5. **infra/**: Infrastructure-as-code for local development and deployment

This structure aligns with **Principle IV (DRY)** by sharing proto/schema definitions, and **Principle II (Library-First)** by using official SDKs rather than custom implementations.

## Complexity Tracking

*Fill ONLY if Constitution Check has violations that must be justified*

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| Multiple service projects (gateway + 2 providers) | Architecture requires language-agnostic provider contract validation; cannot demonstrate cross-language compatibility with single language | Single Python implementation would not validate that provider contract works across Go/Rust/future languages |
| Provider registry abstraction | Dynamic provider discovery from config; support hot-reloading and multi-provider aggregation | Hard-coding provider connections would make system inflexible and violate requirement FR-002 (dynamic capability aggregation) |

**Justification for Multi-Service Architecture**:

The specification explicitly requires demonstrating a language-agnostic provider contract (FR-016, FR-017) with providers in Go and Rust. This necessitates three separate projects. A simpler single-service approach would fail to validate the core architectural requirement: that providers can be implemented in any language and dynamically discovered by the gateway.

The complexity is inherent to the problem domain (microservice orchestration) rather than over-engineering.
